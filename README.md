<p align="justify">

# Deep Reinforcement Learning aplicado a Super Mario Bros

Este proyecto tiene como objetivo el dise침o, entrenamiento y validaci칩n de un agente inteligente capaz de superar de manera aut칩noma los niveles del videojuego **Super Mario Bros**, mediante t칠cnicas de *Aprendizaje por Refuerzo Profundo (Deep Reinforcement Learning)*.

El entorno elegido es `SuperMarioBros-v0`, adaptado desde la librer칤a `gym-super-mario-bros`, y el algoritmo central utilizado para el entrenamiento ha sido **Proximal Policy Optimization (PPO)**, uno de los m칠todos m치s robustos y eficientes de la familia *policy gradient*.

Este repositorio contiene todo lo necesario para **reproducir, analizar y extender** el entrenamiento de agentes tanto en niveles individuales como en configuraciones multinivel.

</p>

---

## 游 Motivaci칩n

<p align="justify">

Super Mario Bros constituye un entorno ideal para el aprendizaje por refuerzo profundo debido a su equilibrio entre complejidad, exploraci칩n, planificaci칩n y control fino. Entrenar un agente para resolver estos niveles implica abordar m칰ltiples desaf칤os:

- Entornos visuales en tiempo real
- Acciones discretas combinadas (saltar, correr, avanzar)
- Recompensas esparsas o retardadas
- Obst치culos mortales y enemigos m칩viles
- Estructuras topol칩gicas variadas (plataformas, t칰neles, castillos)

La soluci칩n propuesta explora tanto la optimizaci칩n individual por nivel como el entrenamiento robusto generalista en entornos vectorizados.

</p>

---

## 丘뙖잺 Arquitectura y dise침o del entorno

<p align="justify">

El entorno base fue modificado mediante una serie de **wrappers personalizados** que permiten mejorar la calidad del entrenamiento y adaptarlo al algoritmo PPO:

- **Procesamiento de im치genes**: cada frame se convierte a escala de grises, se redimensiona a 84칑84 y se normaliza.
- **Apilamiento de frames**: se combinan los 칰ltimos 4 frames en un tensor (4, 84, 84), lo que introduce informaci칩n temporal al agente.
- **Repetici칩n de acciones (ActionRepeat)**: la acci칩n seleccionada se ejecuta varios pasos seguidos (par치metro configurable por nivel), simulando el comportamiento humano y estabilizando la pol칤tica.
- **Funci칩n de recompensa personalizada**:
  - Recompensa positiva al alcanzar la bandera.
  - Penalizaci칩n por inactividad o ca칤das.
  - Bonificaci칩n por recoger monedas o ganar altura vertical (eje Y).
  - Penalizaci칩n si se muere sin llegar al objetivo.

Esta configuraci칩n permite al agente aprender no s칩lo a avanzar, sino a moverse estrat칠gicamente para superar enemigos, evitar peligros y aprovechar el dise침o del nivel.

</p>

---

## 游뱄 Algoritmo de entrenamiento

<p align="justify">

El algoritmo **Proximal Policy Optimization (PPO)** ha sido elegido por sus propiedades:

- Estabilidad en la actualizaci칩n de la pol칤tica mediante *clipping*
- Soporte nativo para pol칤ticas estoc치sticas sobre espacios de acci칩n discretos
- Eficiencia computacional en entornos visuales gracias al uso de mini-lotes y vectorizaci칩n

Se ha implementado con la librer칤a **Stable Baselines 3**, sobre pol칤tica tipo `CnnPolicy`, con logs de entrenamiento integrados en TensorBoard.

Los hiperpar치metros utilizados han sido:

 W-S | n_stes | n_epochs | batch_size |  lr  | gamma | lambda | ent_coeff | clip_range | n_envs | grad_clip_norm | N_FRAMES_REPEAT 
:---:|:------:|:--------:|:----------:|:----:|:-----:|:------:|:---------:|:----------:|:------:|:--------------:|:--------------:
 1-1 |  128   |    8     |     64     |2.5e-4|  0.9  |  0.98  |   0.01    |     0.2    |   16   |      0.5       |       4
 1-2 |  128   |    8     |     64     |2.5e-4|  0.95 |  0.95  |   0.01    |     0.2    |   16   |      0.5       |       8
 1-3 |  128   |    8     |     32     |2.5e-4|  0.95 |  0.98  |   0.02    |     0.2    |   16   |      0.5       |       4
 1-4 |  128   |    8     |     64     |2.5e-4|  0.9  |  0.95  |   0.01    |     0.2    |   16   |      0.5       |       4

</p>

---

## 游빍 Experimentos y niveles entrenados

<p align="justify">

Se han realizado entrenamientos espec칤ficos para los **cuatro niveles del Mundo 1** de Super Mario Bros, obteniendo resultados exitosos:

### Nivel 1-1 (Terreno inicial)
- Dise침o horizontal, sin grandes obst치culos
- Entrenamiento completado en 2.1M pasos
- Comportamiento aprendido: correr y saltar sin detenerse, evitando enemigos innecesarios

<img src="media/sing/world1-stage1.gif" width="200">

### Nivel 1-2 (Subterr치neo)
- Scroll r치pido, t칰neles y enemigos continuos
- `action_repeat` incrementado a 8 para mantener aceleraci칩n
- Pol칤tica aprendida: desplazamiento constante con saltos espaciados

<img src="media/sing/world1-stage2.gif" width="200">

### Nivel 1-3 (Puentes elevados)
- Plataformas suspendidas, enemigos voladores
- A침adida recompensa por ascenso en eje Y
- Mayor dificultad, requiri칩 3.9M pasos

<img src="media/sing/world1-stage3.gif" width="200">

### Nivel 1-4 (Castillo y jefe Bowser)
- Obst치culos lineales y enemigo final
- Resuelto con un modelo entrenado de forma m치s directa (entrenamiento corto)

<img src="media/sing/world1-stage4.gif" width="200">

</p>

---

## 游깷 Entrenamiento robusto multi-nivel

<p align="justify">

Adem치s del enfoque por nivel, se dise침칩 un agente **multinivel** capaz de enfrentarse a todos los niveles del Mundo 1 simult치neamente:

- Se ejecutaron 16 entornos paralelos (4 r칠plicas por nivel) con `SubprocVecEnv`
- Se ajust칩 el par치metro `action_repeat` por nivel:
  - 1-1: 4
  - 1-2: 8
  - 1-3: 2
  - 1-4: 4
- Se incorpor칩 una recompensa vertical para facilitar la exploraci칩n a칠rea en 1-3

Este enfoque logr칩 **generalizar parcialmente**, alcanzando 칠xito completo en 1-1, 1-2 y 1-4. Sin embargo, el nivel 1-3 se mantuvo como un caso dif칤cil debido a su dise침o estructural de plataformas encadenadas.

<p align="left">
  <img src="media/rb/world1-stage1.gif" width="200">
  <img src="media/rb/world1-stage2.gif" width="200">
  <img src="media/rb/world1-stage3.gif" width="200">
  <img src="media/rb/world1-stage4.gif" width="200"><br/>
</p>

</p>

## 游늳 Evaluaci칩n

<p align="justify">

La evaluaci칩n se ha realizado en dos fases:

- **Cuantitativa**: usando `evaluate_policy()` para obtener m칠tricas de recompensa media y desviaci칩n est치ndar.
- **Cualitativa**: renderizando el entorno sin wrappers vectorizados, observando el comportamiento del agente a 60 FPS reales.

Se incluyen v칤deos grabados, logs de TensorBoard y los modelos entrenados en la carpeta `./best_model_robust`.

</p>

---

## 游대 Reproducibilidad

<p align="justify">

Este proyecto proporciona:

- Todos los **scripts** de entrenamiento y evaluaci칩n (`train_robust.py`, `eval_robust.py`)
- Jupyter Notebook con ejemplo de entrenamiento individual (`mario_rl.ipynb`)
- Los par치metros utilizados para cada entorno (`LEVEL_ACTION_REPEAT`, etc.)
- Los modelos guardados y configuraciones de evaluaci칩n
- Registro de entrenamiento en TensorBoard (`./train/`)

Se puede obtener un archivo comprmido con todos los modelos resultantes y datos para poder obervar la evoluci칩n de los entrenamiento en el siguiente enlace: https://drive.google.com/file/d/1c3q6V3ezc4rBDGDn7XPWHbOVCe6nXIcM/view?usp=sharing

</p>

## 游닄 Recursos utilizados

- [`gym-super-mario-bros`](https://github.com/Kautenja/gym-super-mario-bros)
- [`Stable-Baselines3`](https://stable-baselines3.readthedocs.io/)
- [`Gymnasium`](https://gymnasium.farama.org/)

---