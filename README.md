<p align="justify">

# üéÆ Deep Reinforcement Learning aplicado a Super Mario Bros

Este proyecto tiene como objetivo el dise√±o, entrenamiento y validaci√≥n de un agente inteligente capaz de superar de manera aut√≥noma los niveles del videojuego **Super Mario Bros**, mediante t√©cnicas de *Aprendizaje por Refuerzo Profundo (Deep Reinforcement Learning)*.

El entorno elegido es `SuperMarioBros-v0`, adaptado desde la librer√≠a `gym-super-mario-bros`, y el algoritmo central utilizado para el entrenamiento ha sido **Proximal Policy Optimization (PPO)**, uno de los m√©todos m√°s robustos y eficientes de la familia *policy gradient*.

Este repositorio contiene todo lo necesario para **reproducir, analizar y extender** el entrenamiento de agentes tanto en niveles individuales como en configuraciones multinivel.

</p>

---

## üß† Motivaci√≥n

<p align="justify">

Super Mario Bros constituye un entorno ideal para el aprendizaje por refuerzo profundo debido a su equilibrio entre complejidad, exploraci√≥n, planificaci√≥n y control fino. Entrenar un agente para resolver estos niveles implica abordar m√∫ltiples desaf√≠os:

- Entornos visuales en tiempo real
- Acciones discretas combinadas (saltar, correr, avanzar)
- Recompensas esparsas o retardadas
- Obst√°culos mortales y enemigos m√≥viles
- Estructuras topol√≥gicas variadas (plataformas, t√∫neles, castillos)

La soluci√≥n propuesta explora tanto la optimizaci√≥n individual por nivel como el entrenamiento robusto generalista en entornos vectorizados.

</p>

---

## ‚öôÔ∏è Arquitectura y dise√±o del entorno

<p align="justify">

El entorno base fue modificado mediante una serie de **wrappers personalizados** que permiten mejorar la calidad del entrenamiento y adaptarlo al algoritmo PPO:

- **Procesamiento de im√°genes**: cada frame se convierte a escala de grises, se redimensiona a 84√ó84 y se normaliza.
- **Apilamiento de frames**: se combinan los √∫ltimos 4 frames en un tensor (4, 84, 84), lo que introduce informaci√≥n temporal al agente.
- **Repetici√≥n de acciones (ActionRepeat)**: la acci√≥n seleccionada se ejecuta varios pasos seguidos (par√°metro configurable por nivel), simulando el comportamiento humano y estabilizando la pol√≠tica.
- **Funci√≥n de recompensa personalizada**:
  - Recompensa positiva al alcanzar la bandera.
  - Penalizaci√≥n por inactividad o ca√≠das.
  - Bonificaci√≥n por recoger monedas o ganar altura vertical (eje Y).
  - Penalizaci√≥n si se muere sin llegar al objetivo.

Esta configuraci√≥n permite al agente aprender no s√≥lo a avanzar, sino a moverse estrat√©gicamente para superar enemigos, evitar peligros y aprovechar el dise√±o del nivel.

</p>

---

## ü§ñ Algoritmo de entrenamiento

<p align="justify">

El algoritmo **Proximal Policy Optimization (PPO)** ha sido elegido por sus propiedades:

- Estabilidad en la actualizaci√≥n de la pol√≠tica mediante *clipping*
- Soporte nativo para pol√≠ticas estoc√°sticas sobre espacios de acci√≥n discretos
- Eficiencia computacional en entornos visuales gracias al uso de mini-lotes y vectorizaci√≥n

Se ha implementado con la librer√≠a **Stable Baselines 3**, sobre pol√≠tica tipo `CnnPolicy`, con logs de entrenamiento integrados en TensorBoard.

Los principales hiperpar√°metros utilizados han sido:

- `learning_rate`: 2.5e-4
- `gamma`: 0.90 o 0.95, dependiendo del nivel
- `ent_coef`: 0.01‚Äì0.02
- `clip_range`: 0.2
- `gae_lambda`: 0.95
- `n_steps`: 128, `batch_size`: 64, `n_epochs`: 8

</p>

---

## üß™ Experimentos y niveles entrenados

<p align="justify">

Se han realizado entrenamientos espec√≠ficos para los **cuatro niveles del Mundo 1** de Super Mario Bros, obteniendo resultados exitosos:

### Nivel 1-1 (Terreno inicial)
- Dise√±o horizontal, sin grandes obst√°culos
- Entrenamiento completado en 2.1M pasos
- Comportamiento aprendido: correr y saltar sin detenerse, evitando enemigos innecesarios

### Nivel 1-2 (Subterr√°neo)
- Scroll r√°pido, t√∫neles y enemigos continuos
- `action_repeat` incrementado a 8 para mantener aceleraci√≥n
- Pol√≠tica aprendida: desplazamiento constante con saltos espaciados

### Nivel 1-3 (Puentes elevados)
- Plataformas suspendidas, enemigos voladores
- A√±adida recompensa por ascenso en eje Y
- Mayor dificultad, requiri√≥ 3.9M pasos

### Nivel 1-4 (Castillo y jefe Bowser)
- Obst√°culos lineales y enemigo final
- Resuelto con un modelo entrenado de forma m√°s directa (entrenamiento corto)

</p>

---

## üåê Entrenamiento robusto multi-nivel

<p align="justify">

Adem√°s del enfoque por nivel, se dise√±√≥ un agente **multinivel** capaz de enfrentarse a todos los niveles del Mundo 1 simult√°neamente:

- Se ejecutaron 16 entornos paralelos (4 r√©plicas por nivel) con `SubprocVecEnv`
- Se ajust√≥ el par√°metro `action_repeat` por nivel:
  - 1-1: 4
  - 1-2: 8
  - 1-3: 2
  - 1-4: 4
- Se incorpor√≥ una recompensa vertical para facilitar la exploraci√≥n a√©rea en 1-3

Este enfoque logr√≥ **generalizar parcialmente**, alcanzando √©xito completo en 1-1, 1-2 y 1-4. Sin embargo, el nivel 1-3 se mantuvo como un caso dif√≠cil debido a su dise√±o estructural de plataformas encadenadas.

</p>

## üìà Evaluaci√≥n

<p align="justify">

La evaluaci√≥n se ha realizado en dos fases:

- **Cuantitativa**: usando `evaluate_policy()` para obtener m√©tricas de recompensa media y desviaci√≥n est√°ndar.
- **Cualitativa**: renderizando el entorno sin wrappers vectorizados, observando el comportamiento del agente a 60 FPS reales.

Se incluyen v√≠deos grabados, logs de TensorBoard y los modelos entrenados en la carpeta `./best_model_robust`.

</p>

---

## üîÅ Reproducibilidad

<p align="justify">

Este proyecto proporciona:

- Todos los **scripts** de entrenamiento y evaluaci√≥n (`train_robust.py`, `eval_robust.py`)
- Jupyter Notebook con ejemplo de entrenamiento individual (`mario_rl.ipynb`)
- Los par√°metros utilizados para cada entorno (`LEVEL_ACTION_REPEAT`, etc.)
- Los modelos guardados y configuraciones de evaluaci√≥n
- Registro de entrenamiento en TensorBoard (`./train/`)

Se puede obtener un archivo comprmido con todos los modelos resultantes y datos para poder obervar la evoluci√≥n de los entrenamiento en el siguiente enlace: 

</p>

## üìö Recursos utilizados

- [`gym-super-mario-bros`](https://github.com/Kautenja/gym-super-mario-bros)
- [`Stable-Baselines3`](https://stable-baselines3.readthedocs.io/)
- [`Gymnasium`](https://gymnasium.farama.org/)

---